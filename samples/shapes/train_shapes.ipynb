{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  1024\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 1024#128\n",
    "    IMAGE_MAX_DIM = 1024#128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/marziehhaghighi/workspace_DL/Mask_RCNN/mask_rcnn_coco.h5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COCO_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_shapes_test1(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "            \n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "#         shape = random.choice([\"square\", \"circle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "    \n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_val.image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': [0], 'shapes': [0, 1, 2, 3]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.source_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.0': 0, 'shapes.1': 1, 'shapes.2': 2, 'shapes.3': 3}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.class_from_source_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb36f426ba8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb45ca529e8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb36f06e358>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb36ef3f5f8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "config.head='def'\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/marziehhaghighi/workspace_DL/Mask_RCNN/mask_rcnn_coco.h5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COCO_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint Path:  /deepmatter/mask_rcnn/logs/shapes2017102802/mask_rcnn_{epoch:04d}.h5\n",
    "config.LEARNING_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marziehhaghighi/.local/lib/python3.6/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/marziehhaghighi/workspace_DL/Mask_RCNN/logs/shapes20200526T1613/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/marziehhaghighi/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marziehhaghighi/.local/lib/python3.6/site-packages/keras/utils/data_utils.py:709: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 334s 3s/step - loss: 2.7032 - rpn_class_loss: 0.1305 - rpn_bbox_loss: 1.8387 - mrcnn_class_loss: 0.2379 - mrcnn_bbox_loss: 0.2907 - mrcnn_mask_loss: 0.2054 - val_loss: 1.3094 - val_rpn_class_loss: 0.0107 - val_rpn_bbox_loss: 0.9011 - val_mrcnn_class_loss: 0.1153 - val_mrcnn_bbox_loss: 0.1941 - val_mrcnn_mask_loss: 0.0882\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marziehhaghighi/.local/lib/python3.6/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/marziehhaghighi/workspace_DL/Mask_RCNN/logs/shapes20200522T1707/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/marziehhaghighi/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 1.5621 - rpn_class_loss: 0.0352 - rpn_bbox_loss: 0.4951 - mrcnn_class_loss: 0.3745 - mrcnn_bbox_loss: 0.2896 - mrcnn_mask_loss: 0.3676 - val_loss: 1.2134 - val_rpn_class_loss: 0.0181 - val_rpn_bbox_loss: 0.5338 - val_mrcnn_class_loss: 0.2678 - val_mrcnn_bbox_loss: 0.2525 - val_mrcnn_mask_loss: 0.1412\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /home/marziehhaghighi/workspace_DL/Mask_RCNN/logs/shapes20200522T1745/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/marziehhaghighi/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 103s 1s/step - loss: 3.3717 - rpn_class_loss: 0.0444 - rpn_bbox_loss: 0.6893 - mrcnn_class_loss: 1.1084 - mrcnn_bbox_loss: 0.7423 - mrcnn_mask_loss: 0.7873 - val_loss: 2.0486 - val_rpn_class_loss: 0.0227 - val_rpn_bbox_loss: 0.4479 - val_mrcnn_class_loss: 0.3854 - val_mrcnn_bbox_loss: 0.6727 - val_mrcnn_mask_loss: 0.5199\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 1.6406 - rpn_class_loss: 0.0213 - rpn_bbox_loss: 0.4161 - mrcnn_class_loss: 0.3187 - mrcnn_bbox_loss: 0.3867 - mrcnn_mask_loss: 0.4978 - val_loss: 1.5165 - val_rpn_class_loss: 0.0220 - val_rpn_bbox_loss: 0.3973 - val_mrcnn_class_loss: 0.2833 - val_mrcnn_bbox_loss: 0.3683 - val_mrcnn_mask_loss: 0.4456\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "Loading weights from  /home/marziehhaghighi/workspace_DL/Mask_RCNN/logs/shapes20200526T1613/mask_rcnn_shapes_0001.h5\n",
      "Re-starting from epoch 1\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "inference_config.head='def'\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-starting from epoch 1\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (1024, 1024, 3)       min:    4.00000  max:  249.00000  uint8\n",
      "image_meta               shape: (16,)                 min:    0.00000  max: 1024.00000  int64\n",
      "gt_class_id              shape: (3,)                  min:    1.00000  max:    3.00000  int32\n",
      "gt_bbox                  shape: (3, 4)                min:   43.00000  max:  847.00000  int32\n",
      "gt_mask                  shape: (1024, 1024, 3)       min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAACmCAYAAAAoEq3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAEoBJREFUeJzt3Xl4VNX9x/HPnckkM1lZQxI2gRgluFANCipIAwWsdtFq+bWWVmwLVX+uta61Lv2puCAWq9YHra32V61LsWpdAMUKFYKkCAqUKgKiLEGEbJNklnv6RyRGkkAIyQxz5v16njzMzL1z7vfOczifOTNz73WMMQIAAHbwxLsAAADQeQh2AAAsQrADAGARgh0AAIsQ7AAAWIRgBwDAIgQ7AAAWIdgBALAIwQ4AgEUIdgAALJIS7wLaw5eRy3lvk1y4tsKJ1bbob2guFn2PPofmDrbPMWMHAMAiBDsAABYh2AEAsAjBDgCARQh2AAAsQrADAGARgh0AAIsQ7AAAWIRgBwDAIgQ7AAAWIdgBALBIQgf7Ddf9Qj6fr83ly5e8Lr/f3yXbHjigv7ZuWtslbQMA0FEJHey/uv4XSk1NbfG41+uVJJWMKlV9fX2sywIAIG4S4upurZl9zwxJ0qLX/y7XdbVx02bt3LlTRYcXKisrUyWjShWurVC33EGqra3VHbfdpDGnjFJqaqo+3blTP/3ZZfpo88caOKC/li6apzm/f0ynTRyvQCCg6Rdern8uKZMkXTj9fP3vhdNUWVmpl199TRdMm6r8gUNb1HNCyXG69ZZfKjs7S5J006/v0MuvLojdCwIAgBJ4xn7JFddIkkaXnq6SUaWqrKzUscccpdO//T8qGVXaYv07Z87WqDETdfzIr+ovT83Vbb++oWlZr149tbRsuUacNE63zpjZtOzoo4p11ZWXasy40zVqzER165bdai05Odm6f/ZdmnL+z3TiKV/Tt84+Vw/cd7dyclpfHwCArpKwM/bWPPvcCwoGg60umzRhnC6YNlUZmRlK8X55t6ura/TSK/MlSWXLynXn7TdLkk4dfZJeeXWBPv10pyTpD489oe999zst2h41coQOGzhAL859sukxY4wKBw9S+YqVnbJvAAC0R8LO2KNPVzb++3ilok9Xyh0bUvXknY23vxZqWs8tDalgUZZmzrlF36s9V8dsHK4f7/yJ/P19TW00NDQ0tnVnjUL37VZKd0/TsvZw5Ojd99aoZFRp09/gI75CqCexKbtujHcJSDL0OeyRkMG+J3SrwlXKScnZ57om21X2celqUIO2hrfKkaPpA37aru28uXiJJk4Yp549e0iSppw7udX1lpS9rcLCwTp1zMlNj5UcN7xd24C9GGgRa/Q5SAka7HvMuuN3eq3/fK3oV67uK3rL84hf3nNy5Jnf7JfyXw1pi7tLz+kFrfvaGpX1WaJNj22TdnjkPefLbwq8V2XKe2FW0/1V767WzFm/1aLXX1LZ4vmKRCKqrKpuUcfu3ZU665wpuuHaK1W+dKFWlS/WDdf/Qo7jdNm+49DVfHBloEUs0OfQnGOMiXcN++XLyP1SkXtm7HsH897M8LCir1RqWHqpwqrXB56lcgpz5Gzz7vN5zdvPzMxQTU2tpMbj5guHDNKPfnxhh/cFHROurYjZu6S9+9uB2ntgfbz7zQdVD+IrFn2PPofmDrbPJfSMfV+MjKKzapSfUSSf0pSuHHUz+cp5P3BA7dx2yw1avuR1rVy+SMcfd6yuuZ7/MGhba7MlZlDoSvQ57M2qX8U3ZyY3KHWET7nuoKbH8s0RWuv5h5zhmXLeafuMdc6CLz7K33NYHbA/DKaINfocWpOQM3bP1ZnyXJ3Z5nITMHJvq1Vft1hOs130KU19zGClLW6c0bfZ/kMBeR46sJk9sC8MwIg1+lzySshgdz70yvmw7e/J3auCyurTQznq02JZrhmsiBrUt7pvV5aIJMMgilijz6EtCRns+2L6RmUuCaqvW9zqckce9XWLtcX5t4y/9Vm7GRyVGRztyjKRhBiIEWv0ueSUkMHuTq+TO72u9WV31ahX6kD51fZH9TnqI78y1WtHt9bbuKNG7h01nVIr7HcggycDLToDfQ77kpDBbsaHZMaHWjzujg7J+aarfFO03zb6ukO1w9kgk8/MHB3HoIlYo89hfxIy2FtjPEbuPTUqMEfKq7Z/8b5HQNnqbgqU/Z+uuV470BYGZsQafS652BPs59fLPzSgnqZ/u5+Tb4pU6VRoUNXhXVgZbMVgiVijz6E9rAh2082Ve3Ot+rnD5Kj9J+xJUZryTKE+8aze5+FvQGdjgEas0eeShxXB7l4fVE5OH2Wq5wE/t7c5TBGFOfwNB4RBErFGn0N7JXywm6ERmfPqVOAe2aHnNx7+NrTx8LcAs3bEDgM1Yo0+lxwSM9g3eBv/1Hh4Wx9/odKU3uHmmg5/q2g8/G1/Z7ZDcuvMwZGBFu1Bn8OBSMhzxXuvagxdd1KDvGMd9XGHHHSb/dxhWudZLE+/bvs8qx2SG4MiYo0+hwOVmDN2ScZn5M6qUV9TLI8OPoj9ylRP009Za9M6oTqg/Ri4EWv0ObslbrBfXKf0/o3HoneWPHOEqpwdit5e0+aZ7ZC8GAwRa/Q5dERCBnvk+d1yb6xVvzbOB99RKfIpzxwu5+Kw3PENndo2sC8M4Ig1+py9HGMO/V+C+zJyv1RkZNln0rCoVLXXMeufOVLN549lGqnHPvbto2bvafJcac8l2B1JWUZalKKUSd0Pvnh0inBtRftPUHCQ9u5vSG6x6Hv0OTR3sH0uIX88p489jSG+F+cZvzxLG08n644My5xd32YT3iuzmm5HLw9KfZudM97oizcIAAAkkIScsSP5MGNHvDBjR6wdbJ9LyO/YAQBA6wh2AAAsQrADAGARgh0AAIsQ7AAAWIRgBwDAIgR7F/F6uZAMACD2kiLYA4GAnnj8Ya1cvkjlSxfqz4/NkSTd/KtrtHZVmZa8+apm3Hqjli6aJ0n64Q8m68k/PdL0/Ob3jxo2VAvnPa9l/1yglcsX6ZKLpjWt98hDs/XQ/fdo4bznVbZ4viTphJLjNP+lv6ps8XyVLZ6v0yaOj9VuAwCSUGKeee4ATRj/VWVnZ+nYktGSpG7dcnT6aRP0jdMnqWRUqerq6vTsX/7YrrY2bvpIE884W6FQSBkZGXrrH69o3oKF+ve69yVJxx5zlEonflvBYFA5Odm6f/Zd+sZZ39O2bRXKy8vVkjfnafiIMaqsrOqy/QUAJK+kmLGvene1jjzicM2+Z4a+c+Y31NAQ0thTT9FTzzyn2tpaua6rR//453a1lZ6erjkP3qsVy97Qm6+9qIL8PB1z9LCm5c8+94KCwaAkadTIETps4AC9OPdJLV/yul6c+6SMMSocPKhL9hMAgKSYsW/YuEnHloxW6dgxmjRhnH590/X6+8vz2lw/EonK4/niPY8/zd90+/9uuk7btlfo/GkXKxqN6qXnn5Lf/8U13GtraptuO3L07ntrVDrxW528RwAAtM76YF9av0vH5/eR+Wy79NRj0gtPS1u26NK335J+/nPdcvetUn299P2zJDeiUHCHtPpf0rVXKBSplIyRvjlR2r1bobodUqZfWrFMl1dvk4YNk046UeMf/4MeDu6QIvWaEqrRvXU7Gjf+xsvS/TMVOmGY9MYbciS9PWKkTlq9Pq6vCbrWX9afGe8SvmTykLnxLgFdjD6H5qwP9mI3orrSE5V67U2ND3i8ijw0S6HFf1PqicfI8/4aRcI1Cq4vV2a3wQoNypYq1ipt2ZtKWbdWbsVWuWvfk5Obpw8uO05pG59VwdW3KOWi6TIfvC/n7bcU7h1QZFC2/Jk+RXv6tfvUftr1lVxJUuacy5R/+y1ycrrJ46RoyPoNciZPVSJcfAcAkHisv7pbVXCHogOytPmcon2ulz7keOWecak2/uaHHd3UfhXd947eCjsqDXCd9wOVSFd3Y/Zkl0S4uht9zi7JeT32A7BLHoVG5sW7DAAAYsL6X8VHHCmc49/vesH15V06WwcAIBasD/ZDSTjLp9/40uNdBgDAYtYHe5Yxyln9abzLkCSZFI8+dqx/yQEAcWR9yvhllP5xTbzLAAAgJqwP9kNJSjCssyMN8S4DAGAxgj2GPA2uTnTD8S4DAGAxgh0AAIsQ7AAAWIRgBwDAItafeS4iR9Hs1HiXAQBATFgf7LscRw2jCuJdBgAAMcFH8TFkUhx94vHGuwwAgMWsn7EfSsJZqbo3ytcCAICuY/2Mvbdxlf/qxniXAQBATFgf7AAAJBOCPYZSdzXorhDnrQcAdB2CHQAAixDsAABYhGAHAMAiBDsAABax/jj2ajkKF/eIdxkAAMSE9cFe7zhq6J8d7zIAAIgJPoqPoUh6ip5JSYt3GQAAi1kf7H5jlL65Kt5lSJLcNK/KPL54lwEAsJj1wZ4lo5w1n8V+w1zsBQAQB9Z/xy5J8gfUd8oMpeUNlolGFNqxSZ88fq16T7pA2cMnKFpXpeD6cqUPKdHG3/xQOSVnKLN4tD557GpJ+tL9tLwhyjvrGnlSA3J8qdq1dK52LXpCkpQ/+UbJjSq190B50tK1Yda58g8YptyvXyyPP0NpP6jT1BtvVfmCRfF8NQAAFkuKYPeeOk7yZ+jDu74rSfIEspRZPFqZxWP04T3flwk3qN/Uu9vVVnjXVn300IUy0bCc1IAGXfpH1a5bolDFRklSWkGRNj04TSZUL48/U/nfuU6bH75EkeqdKnp6m87751Jdf8JYVVYeGl8PAADskhTB7q55T2m5g9TnzKsUXF+umrWLlT6kRFUr58uE6iRJu8v+pl7jf7zfthyfX3lnXaO0giLJuErJ7i1/QVFTsFevek0mVC9JChx2rHw9CtT/J7MlSZ4pdZIxKhw8SOUrVnbNzgIAklpSBLvZvFEf3v1dpReOUOaRJyv3tItUvabtj8ONG5Ucp+m+4/vil+y5p12kSPVObZl1ruRG1f+nv5WT8sU11t3P3yhIjU00bH1fmx6YJkkquu8dLQs7Kg9078zdAwCgidXBbtyBytc6FUQd7ZzlqK5OCgSkdVuydd0jZ+qyn3s19cFT1FDv6s6/Hq6e29M0duZrGnGiV3P+FNDo+16TMdIzkzIU+U+Nrpz5sGaOOkwL3/Rq4TOF+tPl9yp98HBVrXil1e0HN65Sfq8BSh9yvILryyVJmSUl0ur1sXwZAABJxOpgjzY8oAb5VHr0Fl0w4zBJktcr3X/7Lr3weJVOHhrRcysHqmZ3g97+xzbl9u2nTO3S2jJp8YKeent1urZviWrNyqCKCz7TESllmjfjA/3o0bN03rTh6hWdpuCHK9rcvltXrc2PXqHcMy6VN5Alz0+iGrh+g5zJU2WMidGrgFjbUPyfLml30JqiLmkXiY8+h+asDnZPyh9UHz1P77+xWleMfEs/C/9Z+aZC4ySN80j6pbTxl43r+seeo8DY6Tra/4YK3O0quugJbf+8neLP/71Ckv4lnXz0OB2ptfqrM0EZxqiHjHpI0sxvyi+p2+frRyRVOStUefn/S5IKjKtVjk8msGcNAAA6l9XBLs82OdG6/a93gOqVphWOVyPTuulHkXqNiYZbXe8jj1c3+9Kb7s9pqFa42Xf3wIGYPGRuvEtAkqHPJSa7g30vv/N9v+2Fb0k65UVJ0hZPH92Yelmbq0brPYrI0VaPVzNSMzSjnds/g5k6AKCLWX3mOROZJGMK4l0GAAAxY3ewu8Mlw6FlAIDkYXWwdxWfQnI8bf8aHgCAeCHYO8DvBOVJeSneZQAA0ALBDgCARQj2DojKK2Py410GAAAtEOwdEDRZcsPnx7sMAABasDvYne2SUx/vKgAAiBmrT1Dj8T0qE86OdxkAAMSM3TN2AACSDMEOAIBFrA52N3SNjDss3mUAABAzVgc7AADJhmDvgHSnWh7f7+NdBgAALRDsHeBVVI6zNd5lAADQAsEOAIBFrD6OvVFA68PHKMezU5JU6fbUDrd/m2sXprzTdHtztEgNJr3FOjWmu0zk6/J6l3R+uQAAHATLgz0oqVbrXY8c45UkGeORjNPmM1ZEvU232173M8k9opNrBQDg4DnGmHjXsF++jNxDv0h0qXBtRdvvxjoZ/Q3NxaLv0efQ3MH2Ob5jBwDAIgQ7AAAWIdgBALAIwQ4AgEUIdgAALEKwAwBgEYIdAACLEOwAAFiEYAcAwCIEOwAAFiHYAQCwCMEOAIBFCHYAACxCsAMAYBGCHQAAixDsAABYhGAHAMAiBDsAABYh2AEAsAjBDgCARVLiXUB7hGsrnHjXAABAImDGDgCARRxjTLxrAAAAnYQZOwAAFiHYAQCwCMEOAIBFCHYAACxCsAMAYBGCHQAAixDsAABYhGAHAMAiBDsAABYh2AEAsAjBDgCARQh2AAAsQrADAGARgh0AAIsQ7AAAWIRgBwDAIgQ7AAAWIdgBALAIwQ4AgEUIdgAALEKwAwBgEYIdAACLEOwAAFjkv3L6ef706g0rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb21c2794a8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "# image_id=33\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances2(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rois': array([], shape=(0, 4), dtype=int32),\n",
       "  'class_ids': array([], dtype=int32),\n",
       "  'scores': array([], dtype=float32),\n",
       "  'masks': array([], shape=(128, 128, 0), dtype=float64)}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rois': array([[  0,  77,  53, 127],\n",
       "         [ 39,  32,  99,  92],\n",
       "         [ 55,  61,  92,  97]], dtype=int32),\n",
       "  'class_ids': array([2, 2, 2], dtype=int32),\n",
       "  'scores': array([0.99977416, 0.99931455, 0.88724023], dtype=float32),\n",
       "  'masks': array([[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          ...,\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "  \n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          ...,\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "  \n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          ...,\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          ...,\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "  \n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          ...,\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "  \n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          ...,\n",
       "          [False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]])}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.95\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
